{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gutenberg.acquire import load_etext\n",
    "from gutenberg.cleanup import strip_headers\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import numpy as np\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Loads Moby Dick\n",
    "mobydick = strip_headers(load_etext(2701)).strip()\n",
    "text = [mobydick.split('ETYMOLOGY')[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads Pride and Prejudice\n",
    "pride = strip_headers(load_etext(1342)).strip()\n",
    "text.append(pride.split('Austen')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads Sense and Sensibility\n",
    "sense = strip_headers(load_etext(161)).strip()\n",
    "text.append(sense.split('\\n(1811)\\n\\n')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads Huck Finn\n",
    "huckfinn = strip_headers(load_etext(76)).strip()\n",
    "text.append(huckfinn.split('\\nEXPLANATORY\\n\\n')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads tom sawyer\n",
    "toms = strip_headers(load_etext(74)).strip()\n",
    "text.append(toms.split('\\nPREFACE\\n\\n')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads Twenty Thousand Leagues\n",
    "leagues20k = strip_headers(load_etext(164)).strip()\n",
    "text.append(leagues20k.split(' VERNE\\n\\n')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads Ulysses\n",
    "ulysses = strip_headers(load_etext(4300)).strip()\n",
    "text.append(ulysses.split('\\n— I —\\n')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = ['Moby Dick','Pride and Prejudice','Huck Finn','Tom Sawyer','Sense and Sensibility','20K Leagues','Ulysses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[ 1 ]\n",
      "\n",
      "Stately, plump Buck Mulligan came from the stairhead, bearing a bowl of\n",
      "lather on which a mirror and a razor lay crossed. A yellow\n",
      "dressinggown, ungirdled, was sustained gently behind him on the mild\n",
      "morning air. He held the bowl aloft and intoned:\n",
      "\n",
      "—_Introibo ad altare Dei_.\n",
      "\n",
      "Halted, he peered down the dark winding stairs and called out coarsely:\n",
      "\n",
      "—Come up, Kinch! Come up, you fearful jesuit!\n",
      "\n",
      "Solemnly he came forward and mounted the round gunrest. He faced about\n",
      "and blessed gravely thrice the tower, the surrounding land and the\n",
      "awaking mountains. Then, catching sight of Stephen Dedalus, he bent\n",
      "towards him and made rapid crosses in the air, gurgling in his throat\n",
      "and shaking his head. Stephen Dedalus, displeased and sleepy, leaned\n",
      "his arms on the top of the staircase and looked coldly at the shaking\n",
      "gurgling face that blessed him, equine in its length, and at the light\n",
      "untonsured hair, grained and hued like pale oak.\n",
      "\n",
      "Buck Mulligan peeped an instant under the mirror and then\n"
     ]
    }
   ],
   "source": [
    "print(ulysses.split('\\n— I —\\n')[-1][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters = []\n",
    "for i in range(len(text)):\n",
    "    chapters.append(text[i].split('CHAPTER '))\n",
    "chapters[1] = text[1].split('Chapter ') # fix\n",
    "chapters[6] = text[6].split(' ]\\n\\n') # fix\n",
    "allchaps = [y for x in chapters for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes because he never did a thing like that before as ask to get his\n",
      "breakfast in bed with a couple of eggs since the _City Arms_ hotel when\n",
      "he used to be pretending to be laid up with a sick voice doi\n"
     ]
    }
   ],
   "source": [
    "print(chapters[6][-1][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapix = [len(x) for x in chapters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textcounter(chapters,n_features=1000):\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                       max_features=n_features,\n",
    "                                       stop_words='english')\n",
    "    freq = tfidf_vectorizer.fit_transform(chapters)\n",
    "    words = tfidf_vectorizer.get_feature_names()\n",
    "    return freq, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq, words = textcounter(allchaps,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'whale'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[freq.sum(0).argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sio.savemat('/Users/zaharia/books.mat',{'freq':freq.todense(),'words':np.array(words,dtype=np.object),\\\n",
    "                                        'chapix':chapix,'titles':np.array(titles,dtype=np.object)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sepfreq = []\n",
    "sepwords = []\n",
    "for i in range(len(titles)):\n",
    "    tmpfreq, tmpwords = textcounter(chapters[i],10)\n",
    "    sepfreq.append(tmpfreq)\n",
    "    sepwords.append(tmpwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ahab', 'head', 'like', 'man', 'old', 'sea', 'ship', 'time', 'whale', 'ye'],\n",
       " ['bennet',\n",
       "  'bingley',\n",
       "  'darcy',\n",
       "  'did',\n",
       "  'jane',\n",
       "  'know',\n",
       "  'miss',\n",
       "  'sister',\n",
       "  'soon',\n",
       "  'think'],\n",
       " ['dashwood',\n",
       "  'did',\n",
       "  'edward',\n",
       "  'jennings',\n",
       "  'know',\n",
       "  'miss',\n",
       "  'mother',\n",
       "  'said',\n",
       "  'sister',\n",
       "  'willoughby'],\n",
       " ['ain', 'don', 'en', 'going', 'jim', 'like', 'll', 'man', 'old', 'tom'],\n",
       " ['boy', 'boys', 'did', 'don', 'huck', 'joe', 'just', 'little', 'll', 'said'],\n",
       " ['captain',\n",
       "  'conseil',\n",
       "  'land',\n",
       "  'like',\n",
       "  'nautilus',\n",
       "  'ned',\n",
       "  'nemo',\n",
       "  'said',\n",
       "  'sir',\n",
       "  'water'],\n",
       " ['bloom',\n",
       "  'did',\n",
       "  'like',\n",
       "  'man',\n",
       "  'mr',\n",
       "  'old',\n",
       "  'said',\n",
       "  'says',\n",
       "  'stephen',\n",
       "  'time']]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sepwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sepwords = [y for x in sepwords for y in x]\n",
    "sepfreq = [y.todense() for x in sepfreq for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[matrix([[0.        , 0.        , 0.20375414, 0.        , 0.95255841,\n",
       "          0.        , 0.        , 0.12328658, 0.10895166, 0.15506992]]),\n",
       " matrix([[0.        , 0.5349729 , 0.38887602, 0.42730756, 0.40103169,\n",
       "          0.19242517, 0.1155218 , 0.22145811, 0.14678136, 0.31336889]]),\n",
       " matrix([[0.        , 0.32073785, 0.49372279, 0.27125802, 0.        ,\n",
       "          0.14832856, 0.        , 0.74684856, 0.        , 0.        ]]),\n",
       " matrix([[0.        , 0.15251339, 0.46953819, 0.64492668, 0.        ,\n",
       "          0.56425123, 0.14820151, 0.        , 0.        , 0.        ]]),\n",
       " matrix([[0.        , 0.19036922, 0.73260471, 0.        , 0.17124776,\n",
       "          0.35215292, 0.        , 0.35462476, 0.31339142, 0.2230236 ]]),\n",
       " matrix([[0.        , 0.        , 0.68589628, 0.18842037, 0.20041196,\n",
       "          0.        , 0.21649115, 0.        , 0.36676328, 0.5220109 ]]),\n",
       " matrix([[0.        , 0.        , 0.26172174, 0.35948371, 0.30588979,\n",
       "          0.23588626, 0.74347095, 0.31672267, 0.        , 0.        ]]),\n",
       " matrix([[0.        , 0.14490458, 0.25093867, 0.21446308, 0.03258744,\n",
       "          0.60311368, 0.59843322, 0.10122451, 0.26836445, 0.25464049]])]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sepfreq[1:10][1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sio.savemat('/Users/zaharia/books.mat',{'freq':sepfreq,'words':np.array(sepwords,dtype=np.object),\\\n",
    "                                        'chapix':chapix,'titles':np.array(titles,dtype=np.object)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "409"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(allchaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
