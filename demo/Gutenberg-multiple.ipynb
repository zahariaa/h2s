{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gutenberg.acquire import load_etext\n",
    "from gutenberg.cleanup import strip_headers\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import numpy as np\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads Moby Dick\n",
    "mobydick = strip_headers(load_etext(2701)).strip()\n",
    "text = [mobydick.split('ETYMOLOGY')[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads Pride and Prejudice\n",
    "pride = strip_headers(load_etext(1342)).strip()\n",
    "text.append(pride.split('Austen')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads Sense and Sensibility\n",
    "sense = strip_headers(load_etext(161)).strip()\n",
    "text.append(sense.split('\\n(1811)\\n\\n')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads Huck Finn\n",
    "huckfinn = strip_headers(load_etext(76)).strip()\n",
    "text.append(huckfinn.split('\\nEXPLANATORY\\n\\n')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads Tom Sawyer\n",
    "toms = strip_headers(load_etext(74)).strip()\n",
    "text.append(toms.split('\\nPREFACE\\n\\n')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads Twenty Thousand Leagues\n",
    "leagues20k = strip_headers(load_etext(164)).strip()\n",
    "text.append(leagues20k.split(' VERNE\\n\\n')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads Ulysses\n",
    "ulysses = strip_headers(load_etext(4300)).strip()\n",
    "text.append(ulysses.split('\\n— I —\\n')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads Wizard of Oz\n",
    "wizard = strip_headers(load_etext(55)).strip()\n",
    "text.append(wizard.split('\\nTHE WONDERFUL WIZARD OF OZ\\n\\n')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads Beowulf\n",
    "#beowulf = strip_headers(load_etext(16328)).strip()\n",
    "#text.append(beowulf.split('\\nBEOWULF.\\n\\n')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = ['Moby Dick','Pride Prejudice','Sense Sensibility','Huck Finn','Tom Sawyer','20K Leagues','Ulysses','Wiz of Oz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Moby Dick',\n",
       " 'Pride Prejudice',\n",
       " 'Sense Sensibility',\n",
       " '20K Leagues',\n",
       " 'Ulysses',\n",
       " 'Wiz of Oz']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(beowulf[:1000])\n",
    "text = [text[i] for i in [0,1,2,5,6,7]]\n",
    "titles = [titles[i] for i in [0,1,2,5,6,7]]\n",
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters = []\n",
    "for i in range(len(text)):\n",
    "    chapters.append(text[i].split('CHAPTER '))\n",
    "# manual fixes\n",
    "chapters[1] = text[1].split('Chapter ')\n",
    "chapters[6-2] = text[6-2].split(' ]\\n\\n')\n",
    "import re\n",
    "chapters[7-2] = re.compile(\"\\n\\n\\n[0-9]{1,2}\\.\\s\").split(text[7-2])[1:]\n",
    "allchaps = [y for x in chapters for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re.compile(\"\\n\\nX{0,1}IX|IV|V?I{0,3}\\.\\s\").split(text[-1])[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0, 150, 212, 263, 310, 329, 351])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chapix = [len(x) for x in chapters]\n",
    "# in matlab: chapix = [0 cumsum(chapix)]\n",
    "chapix = np.insert(np.cumsum(chapix),0,0)\n",
    "chapix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textcounter(chapters,n_features=1000):\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.999, min_df=2,\n",
    "                                       max_features=n_features,\n",
    "                                       stop_words='english')\n",
    "    freq = tfidf_vectorizer.fit_transform(chapters)\n",
    "    words = tfidf_vectorizer.get_feature_names()\n",
    "    return freq, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq, words = textcounter(allchaps,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'whale'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[freq.sum(0).argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sio.savemat('books.mat',{'freq':freq.todense(),'words':np.array(words,dtype=np.object),\\\n",
    "                         'chapix':chapix,'titles':np.array(titles,dtype=np.object)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sepfreq = []\n",
    "sepwords = []\n",
    "for i in range(len(titles)):\n",
    "    tmpfreq, tmpwords = textcounter(chapters[i],15)\n",
    "    sepfreq.append(tmpfreq)\n",
    "    sepwords.append(tmpwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ahab',\n",
       "  'boat',\n",
       "  'captain',\n",
       "  'great',\n",
       "  'head',\n",
       "  'like',\n",
       "  'long',\n",
       "  'man',\n",
       "  'old',\n",
       "  'said',\n",
       "  'sea',\n",
       "  'ship',\n",
       "  'time',\n",
       "  'whale',\n",
       "  'ye'],\n",
       " ['bennet',\n",
       "  'bingley',\n",
       "  'darcy',\n",
       "  'did',\n",
       "  'elizabeth',\n",
       "  'jane',\n",
       "  'know',\n",
       "  'miss',\n",
       "  'mr',\n",
       "  'mrs',\n",
       "  'said',\n",
       "  'sister',\n",
       "  'soon',\n",
       "  'think',\n",
       "  'time'],\n",
       " ['dashwood',\n",
       "  'did',\n",
       "  'edward',\n",
       "  'elinor',\n",
       "  'jennings',\n",
       "  'know',\n",
       "  'marianne',\n",
       "  'miss',\n",
       "  'mother',\n",
       "  'mrs',\n",
       "  'said',\n",
       "  'sister',\n",
       "  'think',\n",
       "  'time',\n",
       "  'willoughby'],\n",
       " ['captain',\n",
       "  'conseil',\n",
       "  'day',\n",
       "  'did',\n",
       "  'feet',\n",
       "  'land',\n",
       "  'like',\n",
       "  'long',\n",
       "  'nautilus',\n",
       "  'ned',\n",
       "  'nemo',\n",
       "  'said',\n",
       "  'sea',\n",
       "  'sir',\n",
       "  'water'],\n",
       " ['bloom',\n",
       "  'did',\n",
       "  'eyes',\n",
       "  'good',\n",
       "  'hand',\n",
       "  'know',\n",
       "  'like',\n",
       "  'man',\n",
       "  'mr',\n",
       "  'old',\n",
       "  'said',\n",
       "  'says',\n",
       "  'stephen',\n",
       "  'time',\n",
       "  'yes'],\n",
       " ['asked',\n",
       "  'came',\n",
       "  'girl',\n",
       "  'good',\n",
       "  'great',\n",
       "  'green',\n",
       "  'lion',\n",
       "  'little',\n",
       "  'oz',\n",
       "  'said',\n",
       "  'scarecrow',\n",
       "  'tin',\n",
       "  'toto',\n",
       "  'witch',\n",
       "  'woodman']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sepwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sepwords = [y for x in sepwords for y in x]\n",
    "sepfreq = [y.todense() for x in sepfreq for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[matrix([[0.        , 0.        , 0.        , 0.22636679, 0.        ,\n",
       "          0.18968858, 0.        , 0.        , 0.88680139, 0.28645535,\n",
       "          0.        , 0.        , 0.11477586, 0.10143051, 0.14436513]]),\n",
       " matrix([[0.        , 0.03037384, 0.        , 0.13388089, 0.43728575,\n",
       "          0.31786646, 0.23385453, 0.34928032, 0.32780248, 0.5082578 ,\n",
       "          0.15728794, 0.09442728, 0.18101941, 0.11997878, 0.25614709]]),\n",
       " matrix([[0.        , 0.        , 0.        , 0.49340825, 0.26859747,\n",
       "          0.41346131, 0.23505105, 0.22716127, 0.        , 0.        ,\n",
       "          0.1242157 , 0.        , 0.62543798, 0.        , 0.        ]]),\n",
       " matrix([[0.        , 0.        , 0.        , 0.26128346, 0.14223531,\n",
       "          0.43789539, 0.24894172, 0.60146422, 0.        , 0.        ,\n",
       "          0.5262256 , 0.13821401, 0.        , 0.        , 0.        ]]),\n",
       " matrix([[0.        , 0.        , 0.        , 0.16996574, 0.18504906,\n",
       "          0.71213094, 0.1619374 , 0.        , 0.16646197, 0.        ,\n",
       "          0.34231146, 0.        , 0.34471423, 0.30463321, 0.21679086]]),\n",
       " matrix([[0.        , 0.62023886, 0.20848564, 0.        , 0.        ,\n",
       "          0.50908903, 0.14470751, 0.13985022, 0.14875067, 0.        ,\n",
       "          0.        , 0.16068504, 0.        , 0.2722207 , 0.38744929]]),\n",
       " matrix([[0.        , 0.10278552, 0.10365025, 0.15101827, 0.        ,\n",
       "          0.25309755, 0.14388491, 0.3476381 , 0.2958102 , 0.        ,\n",
       "          0.22811341, 0.7189723 , 0.30628611, 0.        , 0.        ]]),\n",
       " matrix([[0.        , 0.        , 0.41125652, 0.11984004, 0.13047504,\n",
       "          0.2259503 , 0.        , 0.19310693, 0.0293424 , 0.07582565,\n",
       "          0.54305587, 0.53884148, 0.09114462, 0.24164083, 0.22928349]])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sepfreq[1:10][1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sio.savemat('books.mat',{'freq':sepfreq,'words':np.array(sepwords,dtype=np.object),\\\n",
    "                         'chapix':chapix,'titles':np.array(titles,dtype=np.object)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
